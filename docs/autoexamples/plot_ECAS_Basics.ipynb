{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nDemo: ECAS/Ophidia simple commands examples\n===========================================\n\nFirst of all import PyOphidia modules and connect to server (connection details are inferred from the ECAS environment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n# coding: utf-8\n\n# # Demo:  ECAS/Ophidia simple commands examples\n\n# First of all import PyOphidia modules and connect to server (connection details are inferred from the ECAS environment)\n\n# In[ ]:\n\n\nfrom PyOphidia import cube, client\ncube.Cube.setclient(read_env=True)\n\n\n# Create a datacube from the NetCDF file:\n# - The file is **/public/data/ecas_training/tos_O1_2001-2002.nc**\n# - The variable to be imported is **tos**\n# - Data should be arranged in order to operate on time series (**time** dimension) \n# \n# **Note: We are not directly reading the file from the Notebook**\n\n# In[ ]:\n\n\nmycube = cube.Cube.importnc(\n                src_path='/public/data/ecas_training/tos_O1_2001-2002.nc',\n                measure='tos',\n                imp_dim='time',\n                ioserver='ophidiaio_memory',\n                ncores=2,\n                description=\"Imported cube\"\n        )\n\n\n# Check the datacubes available in the virtual file system\n\n# In[ ]:\n\n\ncube.Cube.list(level=2)\n\n\n# To get the list of arguments and default values we can use the python *help()* command can be used\n\n# In[ ]:\n\n\nhelp(cube.Cube.list)\n\n\n# Inspect the cube and its dimensions structure\n\n# In[ ]:\n\n\nmycube.info()\n\n\n# Subset the datacube over space (lat and lon) and time\n# \n# **Note: each instance method produces a new datacube object**\n\n# In[ ]:\n\n\nmycube2 = mycube.subset(\n                subset_dims=\"lat|lon|time\",\n                subset_filter=\"-80:30|30:120|151:240\",\n                subset_type=\"coord\",\n                ncores=2,\n                description=\"Subsetted cube\"\n        )\n\n\n# Inspect the new cube; dimensions have been changed\n\n# In[ ]:\n\n\nmycube2.info()\n\n\n# But what does the datacube actually contain at this point? We can use the explore method to check the content. \n\n# In[ ]:\n\n\nmycube2.explore(limit_filter=1)\n\n\n# We can then compute the maximum value over the time series for each point in the spatial domain\n\n# In[ ]:\n\n\nmycube3 = mycube2.reduce(\n                    operation='max',\n                    ncores=2,\n                    description=\"Reduced cube\"\n                )\n\n\n# In the new cube the time dimension is be \"collapesed\"\n\n# In[ ]:\n\n\nmycube3.info()\n\n\n# We can now reorganize the data structure by making the longitude dimension an array-oriented dimension\n\n# In[ ]:\n\n\nmycube4 = mycube3.rollup(\n                    ncores=2,\n                    description=\"Rollup cube\"\n                )\n\n\n# The new cube will now have *lon* has an array-dimension\n\n# In[ ]:\n\n\nmycube4.info()\n\n\n# Each operation executed creates a new datacube on the framework (datacubes are not overwritten)\n\n# In[ ]:\n\n\ncube.Cube.list(level=2)\n\n\n# Let's export the data into a Python-friendly structure. \n# \n# **Note: this is the first time we move data from the server-side to the Notebook**\n\n# The structure looks something like this\n# \n# <img src=\"imgs/export_array.png\" alt=\"Export Array\" width=\"800\">\n# \n# \n\n# In[ ]:\n\n\ndata = mycube4.export_array()\n\nfrom IPython.lib.pretty import pprint\npprint(data)\n\n\n# The data exported in the Python structure can be used to create a map (note the definition of a Python function)\n\n# In[ ]:\n\n\n# get_ipython().run_line_magic('matplotlib', 'inline')\n\ndef plotData(data):\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.basemap import Basemap, cm\n    import numpy as np\n\n    lat = data['dimension'][0]['values'][:]\n    lon = data['dimension'][1]['values'][:]\n    var = data['measure'][0]['values'][:]\n\n    fig = plt.figure(figsize=(15, 15), dpi=100)\n    ax  = fig.add_axes([0.1,0.1,0.8,0.8])\n\n    map = Basemap(projection='cyl',llcrnrlat= -90,urcrnrlat= 90, llcrnrlon=0,urcrnrlon=360, resolution='c')\n\n    map.drawcoastlines()\n    map.drawparallels(np.arange( -90, 90,30),labels=[1,0,0,0])\n    map.drawmeridians(np.arange(-180,180,30),labels=[0,0,0,1])\n\n    x, y = map(*np.meshgrid(lon,lat))\n\n    cnplot = map.contourf(x,y,var,np.arange(270,305,0.5),cmap=plt.cm.jet)\n    cbar = map.colorbar(cnplot,location='right',format='%.2f')\n\n    plt.title('Sea Surface Temperature (deg K)')\n    plt.show()\n    \nplotData(data)\n\n\n# #### What If we want to consider the whole spatial domain and specify a subset only on the time range? \n# \n# We can perform the new set of operations on *mycube* object, without the need to re-import the dataset from the file. Note that we are providing the time range in human-readable form\n\n# In[ ]:\n\n\nnewMycube2 = mycube.subset(\n                subset_dims=\"time\",\n                subset_filter=\"2001-01-01_2001-12-31\",\n                subset_type=\"coord\",\n                time_filter=\"yes\",\n                ncores=2,\n                description=\"New subsetted cube\"\n        )\n\nnewMycube2.info()\n\n\n# We can the rerun the same operations on the new cube ...\n\n# In[ ]:\n\n\nnewMycube3 = newMycube2.reduce(\n                    operation='max',\n                    ncores=2,\n                    description=\"New reduced cube\"\n                )\n\nnewMycube4 = newMycube3.rollup(\n                    ncores=2,\n                    description=\"New rollup cube\"\n                )\n\n\n# ... and plot the new datacube values on a map using the function *plotData*\n\n# In[ ]:\n\n\ndata = newMycube4.export_array()\nplotData(data)\n\n\n# #### What if we want to get the *minimum* instead of the maximum value?\n# \n# Again we can perform the new set of operations on *newMycube2* object, without the need to re-import or subset the dataset again\n\n# In[ ]:\n\n\nnewNewMycube3 = newMycube2.reduce(\n                    operation='min',\n                    ncores=2,\n                    description=\"New reduced cube2\"\n                )\n\nnewNewMycube4 = newNewMycube3.rollup(\n                    ncores=2,\n                    description=\"New rollup cube2\"\n                )\n\n\n# ... and plot the new datacube values on a map using the function *plotData*\n\n# In[ ]:\n\n\ndata = newNewMycube4.export_array()\nplotData(data)\n\n\n# Our workspace now contains several datacubes from the experiments just run. Once done, we can clear the space before moving to other notebooks. \n# \n# **Note: the *client.submit* is exploiting the underlying *PyOphidia client class* to submit commands in terminal-like syntax.**\n\n# In[ ]:\n\n\ncube.Cube.deletecontainer(container='tos_O1_2001-2002.nc',force='yes')\n\n\n# The virtual file system should now be \"clean\"\n\n# In[ ]:\n\n\ncube.Cube.list(level=2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}